{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path\n",
    "with path.Path('../Utils'):\n",
    "    import constant as c\n",
    "    import functions as func\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import javalang\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "import glob\n",
    "import numpy as np\n",
    "import pprint\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import recall_score\n",
    "from imblearn.metrics import specificity_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Convolution1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, concatenate\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(obj, name ):\n",
    "    with open('DATA/dictionarys/'+ name + '.pkl', 'wb') as ff:\n",
    "        pickle.dump(obj, ff, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_dict(name ):\n",
    "    with open('DATA/dictionarys/' + name + '.pkl', 'rb') as ff:\n",
    "        return pickle.load(ff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words(previous, current, verbose= False):\n",
    "    max_length = 0\n",
    "    words = np.array([], dtype = 'object')\n",
    "    for index, row in previous.iterrows():\n",
    "        previous_file = 'DATA/embedded data'+row.pathfolder[7:]\n",
    "        #print(previous_file)\n",
    "        with open(previous_file, 'rb') as f: \n",
    "            m = 0\n",
    "            for line in f:\n",
    "                m += 1\n",
    "                t = line.split()[0].decode('utf-8')\n",
    "\n",
    "                words = np.append(words, t)\n",
    "\n",
    "        if m > max_length:\n",
    "            max_length = m\n",
    "            max_file = previous_file\n",
    "\n",
    "    \n",
    "\n",
    "    for index, row in current.iterrows():\n",
    "        current_file = 'DATA/embedded data'+row.pathfolder[7:]\n",
    "        #print(previous_file)\n",
    "        with open(current_file, 'rb') as f:\n",
    "            m = 0\n",
    "            for line in f:\n",
    "                m +=1\n",
    "                t = line.split()[0].decode('utf-8')\n",
    "\n",
    "                words = np.append(words, t)\n",
    "\n",
    "        if m > max_length:\n",
    "            max_length = m\n",
    "            max_file = current_file\n",
    "\n",
    "    #k = words\n",
    "    if verbose:\n",
    "        print(len(words))           \n",
    "        words = np.unique(words)\n",
    "        print(words.shape)\n",
    "        print('max lenght =' + str(max_length))\n",
    "    \n",
    "    \n",
    "    print('convert_words finish')\n",
    "    return max_length, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_new_embedded(previous, current, dictionary, max_length):\n",
    "    for index, row in previous.iterrows():\n",
    "        vectors = np.array([])\n",
    "        previous_file = 'DATA/embedded data'+row.pathfolder[7:]\n",
    "        #print(row.pathfolder)\n",
    "        Path(Path('DATA/new_embedded data'+row.pathfolder[7:]+'.embed').parent).mkdir(parents=True, exist_ok=True)\n",
    "        Path(Path('DATA/new_embedded data'+row.pathfolder[7:]+'.embed')).touch()\n",
    "        #print(previous_file)\n",
    "        with open(previous_file, 'rb') as f:\n",
    "            for line in f:\n",
    "                t = line.split()[0].decode('utf-8')\n",
    "                #print(t, '=>', dictionary[t])\n",
    "                vectors = np.append(vectors, dictionary[t])\n",
    "        vectors = vectors.astype('int32')\n",
    "        if vectors.shape[0] < max_length:\n",
    "            new_lenght = max_length - vectors.shape[0]\n",
    "            vectors = np.append(vectors, np.zeros(new_lenght))\n",
    "        np.savetxt('DATA/new_embedded data'+row.pathfolder[7:]+'.embed', vectors, delimiter=',', fmt='%i')\n",
    "\n",
    "    for index, row in current.iterrows():\n",
    "        vectors = np.array([])\n",
    "        current_file = 'DATA/embedded data'+row.pathfolder[7:]\n",
    "        #print(row.pathfolder)\n",
    "        Path(Path('DATA/new_embedded data'+row.pathfolder[7:]+'.embed').parent).mkdir(parents=True, exist_ok=True)\n",
    "        #print(current_file)\n",
    "        with open(current_file, 'rb') as f:\n",
    "            for line in f:\n",
    "                t = line.split()[0].decode('utf-8')\n",
    "                #print(t, '=>', dictionary[t])\n",
    "                vectors = np.append(vectors, dictionary[t])\n",
    "        vectors = vectors.astype('int32')\n",
    "        if vectors.shape[0] < max_length:\n",
    "            new_lenght = max_length - vectors.shape[0]\n",
    "            vectors = np.append(vectors, np.zeros(new_lenght))\n",
    "        np.savetxt('DATA/new_embedded data'+row.pathfolder[7:]+'.embed', vectors, delimiter=',', fmt='%i')\n",
    "        \n",
    "    print('do_new_embedded finish')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token(ant_token, version_tuple, verbose = False):\n",
    "    if verbose:\n",
    "        print(ant_token)\n",
    "        #print('Tuple versions ->', version_tuple)\n",
    "    previous = ant_token[ant_token['pathfolder'].str.contains((version_tuple[0]))].reset_index().drop(columns = ['index'])\n",
    "    current = ant_token[ant_token['pathfolder'].str.contains((version_tuple[1]))].reset_index().drop(columns = ['index'])\n",
    "    \n",
    "    print('Conver words starting...')\n",
    "    max_length, words = convert_words(previous, current)\n",
    "    \n",
    "    tokens = np.array(random.sample(range(words.shape[0]), words.shape[0]))\n",
    "    dictionary = dict(zip(words, tokens))\n",
    "    print('Conver words starting...')\n",
    "\n",
    "    #save_dict(dictionary, 'ant-1.3 ant-1.4')\n",
    "    \n",
    "    print('Create new embedded files starting...')\n",
    "    do_new_embedded(previous, current, dictionary, max_length)\n",
    "    \n",
    "    print('finish')\n",
    "    \n",
    "    print('Create Dataset For CNN starting...')\n",
    "    train_X, train_y, test_X, test_y = create_trainset_forCNN(version_tuple, previous, current, verbose)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Create the CNN Model')\n",
    "    model = create_model(train_X, tokens)\n",
    "    \n",
    "    print('Generate Sematic Features')\n",
    "    predicted_train, predicted_test= generate_new_features(model,train_X, train_y, test_X, test_y)\n",
    "    \n",
    "    print('Creating Dataset for Regressor')\n",
    "    ntrain_X, ntrain_y, ntest_X, ntest_y = create_dataset_forClass(previous, current, version_tuple, predicted_train, predicted_test, verbose )\n",
    "    \n",
    "\n",
    "    print('Starting Regressor...')\n",
    "    run_logisticRegression(ntrain_X, ntrain_y, ntest_X, ntest_y, verbose = False, plot = False)\n",
    "    \n",
    "    #run_random_forest(ntrain_X, ntrain_y, ntest_X, ntest_y, verbose = False, plot = False)\n",
    "    \n",
    "    \n",
    "    print('Finished')\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainset_forCNN(version_tuple, previous, current, verbose = False):\n",
    "    global mappeddataset\n",
    "    ant_csv = glob.glob(mappeddataset)\n",
    "    ant_token = []\n",
    "    for i in ant_csv:\n",
    "        tmp = pd.read_csv(i, names=['pathfolder', 'label'])\n",
    "        ant_token.append(tmp)\n",
    "    ant_token =pd.concat(ant_token)\n",
    "    ant_token_sorted = ant_token.sort_values(by ='pathfolder')\n",
    "    #ant_token =ant_token.reset_index().drop(['index'], axis= 1)\n",
    "    ant_token_sorted =ant_token_sorted.reset_index().drop(['index'], axis= 1)\n",
    "    previous = ant_token_sorted[ant_token_sorted['pathfolder'].str.contains(version_tuple[0])].reset_index()\n",
    "    current = ant_token_sorted[ant_token_sorted['pathfolder'].str.contains(version_tuple[1])].reset_index().drop(columns = ['index'])\n",
    "    \n",
    "    vectors = []\n",
    "    for index, row in previous.iterrows():\n",
    "        previous_file = 'DATA/new_embedded data'+row.pathfolder[7:]\n",
    "        #print(previous_file)\n",
    "        with open(previous_file, 'rb') as f: \n",
    "            l = []\n",
    "            for line in f:\n",
    "                l.append(int(line.split()[0]))\n",
    "\n",
    "        vectors.append(l)\n",
    "\n",
    "    train_X = np.array(vectors)\n",
    "    train_y = previous['label'].to_numpy()\n",
    "    \n",
    "    if verbose:\n",
    "        print('train_X shape ->', train_X.shape)\n",
    "    vectors = []\n",
    "    for index, row in current.iterrows():\n",
    "        previous_file = 'DATA/new_embedded data'+row.pathfolder[7:]\n",
    "        #print(previous_file)\n",
    "        with open(previous_file, 'rb') as f: \n",
    "            l = []\n",
    "            for line in f:\n",
    "                l.append(int(line.split()[0]))\n",
    "\n",
    "        vectors.append(l)\n",
    "\n",
    "    test_X = np.array(vectors)\n",
    "    test_y = current['label'].to_numpy()\n",
    "    \n",
    "    if verbose:\n",
    "        print('After create CNN SET train_X shape ->', train_X.shape)\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train_X, tokens, n_filters= 15, filter_size = 5):\n",
    "    \n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(tokens.shape[0], 32, input_length=train_X.shape[1], embeddings_regularizer=tf.keras.regularizers.l2(l=0.0001)))\n",
    "    model.add(\n",
    "            Convolution1D(\n",
    "                filters = 32,\n",
    "                strides=2,\n",
    "                kernel_size=filter_size,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "\n",
    "            ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(\n",
    "            Convolution1D(\n",
    "                filters = 32,\n",
    "                strides=2,\n",
    "                kernel_size=filter_size,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "\n",
    "            ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(\n",
    "            Convolution1D(\n",
    "                filters = 64,\n",
    "                strides=2,\n",
    "                kernel_size=filter_size,\n",
    "                kernel_initializer=\"he_normal\",\n",
    "\n",
    "            ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(Dense( 256,kernel_regularizer= tf.keras.regularizers.l2(l=0.0001), kernel_initializer=\"he_normal\"))\n",
    "    model.add(Dropout(.50))\n",
    "    model.add(Dense( 256,kernel_regularizer= tf.keras.regularizers.l2(l=0.0001), kernel_initializer=\"he_normal\"))\n",
    "    model.add(Dropout(.50))\n",
    "    model.add(Dense( 256,kernel_regularizer= tf.keras.regularizers.l2(l=0.0001), kernel_initializer=\"he_normal\"))\n",
    "    model.add(Dropout(.50))\n",
    "    model.add(Dense( 256,kernel_regularizer= tf.keras.regularizers.l2(l=0.0001), kernel_initializer=\"glorot_normal\"))\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_features(model, train_X, train_y, test_X, test_y):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_X)\n",
    "    X_train = scaler.transform(train_X)\n",
    "    y_train = train_y\n",
    "    X_test = scaler.transform(test_X)\n",
    "    y_test = test_y\n",
    "    \n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0.0001, patience=10)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0015), loss='sparse_categorical_crossentropy',  metrics=['accuracy'] )\n",
    "    history = model.fit(train_X, train_y,epochs=20 ,callbacks=[callback], batch_size = 256,verbose=0)\n",
    "    \n",
    "    predicted_train = model.predict(train_X)\n",
    "    predicted_test = model.predict(test_X)\n",
    "    \n",
    "    return predicted_train, predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_forClass(previous, current, version_tuple,predicted_train, predicted_test, verbose = False):\n",
    "    global DS3\n",
    "    global classicdataset\n",
    "    features = ['wmc', 'dit', 'noc', 'cbo', 'rfc', 'lcom',\n",
    "       'ca', 'ce', 'npm', 'lcom3', 'loc', 'dam', 'moa', 'mfa', 'cam', 'ic',\n",
    "       'cbm', 'amc', 'max_cc', 'avg_cc']\n",
    "    ant_dataframe = []\n",
    "\n",
    "    for filename in os.listdir(classicdataset):\n",
    "        tmp = pd.read_csv(join(classicdataset, filename))\n",
    "        ant_dataframe.append(tmp)\n",
    "    ant_dataframe = pd.concat(ant_dataframe)\n",
    "    previous_old = ant_dataframe.loc[ant_dataframe.version == float(version_tuple[0][-3:])].reset_index().drop(columns=['index'])\n",
    "    current_old = ant_dataframe.loc[ant_dataframe.version == float(version_tuple[1][-3:])].reset_index().drop(columns=['index'])\n",
    "    previous_old = previous_old.sort_values(by=['name']).reset_index().drop(columns=['index'])\n",
    "    current_old = current_old.sort_values(by=['name']).reset_index().drop(columns=['index'])\n",
    "    \n",
    "    \n",
    "    if DS3:\n",
    "        D = func.create_D(current_old, previous_old, features, 'c')\n",
    "        idx = func.runDS3(D, reg = .5, verbose = False)\n",
    "        previous_old = previous_old.iloc[idx].reset_index().drop(columns=['index'])\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    check = []\n",
    "    ntrain_X = []\n",
    "    ntrain_y = []\n",
    "    for index, i in previous_old.iterrows():\n",
    "        s = i['name']\n",
    "        #print(s)\n",
    "        for indexj, j in previous.iterrows():\n",
    "            sj = j.pathfolder.replace('/', '.')\n",
    "            #print(indexj)\n",
    "            if (s in sj):\n",
    "                #print(indexj)\n",
    "                check.append(s)\n",
    "    #             print(i[features])\n",
    "    #             print(predicted_train[indexj])\n",
    "                ntrain_X.append(np.concatenate((i[features], predicted_train[indexj]), axis = 0))\n",
    "                ntrain_y.append(i['bug'])\n",
    "\n",
    "                break\n",
    "    check = []\n",
    "    ntest_X = []\n",
    "    ntest_y = []\n",
    "    for index, i in current_old.iterrows():\n",
    "        s = i['name']\n",
    "        #print(s)\n",
    "        for indexj, j in current.iterrows():\n",
    "            sj = j.pathfolder.replace('/', '.')\n",
    "            #print(indexj)\n",
    "            if (s in sj):\n",
    "                #print(indexj)\n",
    "                check.append(s)\n",
    "    #             print(i[features])\n",
    "    #             print(predicted_train[indexj])\n",
    "                ntest_X.append(np.concatenate((i[features], predicted_test[indexj]), axis = 0))\n",
    "                ntest_y.append(i['bug'])\n",
    "\n",
    "                break\n",
    "    \n",
    "    ntrain_X = np.array(ntrain_X)\n",
    "    ntrain_X.shape\n",
    "    ntrain_y = np.array(ntrain_y)\n",
    "    ntrain_y.shape\n",
    "    \n",
    "    ntest_X = np.array(ntest_X)\n",
    "    ntest_X.shape\n",
    "    ntest_y = np.array(ntest_y)\n",
    "    ntest_y.shape\n",
    "    \n",
    "    return ntrain_X, ntrain_y, ntest_X, ntest_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy, predictions\n",
    "\n",
    "def run_random_forest(train_X, train_y, test_X, test_y, verbose = False, plot = False):\n",
    "    \"\"\"\n",
    "        This function trains and uses model.\n",
    "\n",
    "        :param D: matrix whose non-zero rows corresponds to the representatives of the dataset.\n",
    "        :param p: norm to be used to calculate regularization cost.\n",
    "        :returns: regularization cost.\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_X)\n",
    "    X_train = scaler.transform(train_X)\n",
    "    y_train = train_y\n",
    "    X_test = scaler.transform(test_X)\n",
    "    y_test = test_y\n",
    "    ##PREDICT##\n",
    "    \n",
    "    # Create the random grid\n",
    "    parameters = {\n",
    "                    'n_estimators'      : [200, 300, 320,330,340],\n",
    "                    'max_depth'         : [8, 9],\n",
    "                    'random_state'      : [0],\n",
    "                    'max_features'      : ['auto', 'sqrt', 'log2']\n",
    "                    #'max_features': ['auto'],\n",
    "                    #'criterion' :['gini']\n",
    "                }\n",
    "    \n",
    "    # Random search of parameters, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    clf = GridSearchCV(RandomForestClassifier(), \n",
    "                                parameters, cv= 10, n_jobs = -1)\n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    if verbose:\n",
    "        print('random forest score:', clf.score(X_train, y_train))\n",
    "    print(clf.best_params_)\n",
    "    y_predicted = clf.predict(X_test)\n",
    "    \n",
    "    ##CALCULATE SCORE OF THE MODEL##\n",
    "    if plot:\n",
    "        fig = px.scatter(y_test)\n",
    "        fig.add_trace(go.Scatter(x= list(range(y_test.shape[0])), y= y_predicted))\n",
    "        fig.show()\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "    print()\n",
    "    print()\n",
    "        \n",
    "\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    gmean = geometric_mean_score(y_test, y_predicted, average = 'micro')\n",
    "    #print(cm)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)\n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    #TPR = TP / (TP + FN)\n",
    "    TPR = recall_score(y_test, y_predicted, average='micro')\n",
    "\n",
    "    # Fall out or false positive rate\n",
    "    #FPR = FP / (FP + TN)\n",
    "    FPR = 1 - (specificity_score(y_test, y_predicted, average='micro'))\n",
    "\n",
    "    #Balance\n",
    "    balance = 1 - (np.sqrt((0 - FPR) ** 2 + (1 - TPR) ** 2) / np.sqrt(2))\n",
    "    balance = np.average(balance)\n",
    "\n",
    "    ##F MEASURE##\n",
    "    fmeasure = f1_score(y_test, y_predicted, average = 'micro')\n",
    "    print('F-Measure : ', fmeasure)\n",
    "    print('G-Mean :', gmean)\n",
    "    print('Balance :', balance)\n",
    "############################################################################\n",
    "############################################################################\n",
    "#    'micro':\n",
    "#     Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "#\n",
    "#   'macro':\n",
    "#     Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "############################################################################\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "    '''filename = join(c.MODEL_DIR , 'digits_classifier.joblib.pkl')\n",
    "    _ = joblib.dump(clf, filename, compress=9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logisticRegression(train_X, train_y, test_X, test_y, verbose = False, plot = False):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_X)\n",
    "    X_train = scaler.transform(train_X)\n",
    "    y_train = train_y\n",
    "    X_test = scaler.transform(test_X)\n",
    "    y_test = test_y\n",
    "    \n",
    "    clf = LogisticRegression(solver = 'liblinear')\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    ##PREDICT##\n",
    "    y_predicted = clf.predict(X_test)\n",
    "    print(y_predicted)\n",
    "    ##CALCULATE SCORE OF THE MODEL##\n",
    "    score = clf.score(X_test, y_test)\n",
    "    if True:\n",
    "        print(f'- LogisticRegression score: {score}')\n",
    "    # CONFUCIO MATRIX##\n",
    "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
    "    ##PLOT CONFUSION MATRIX##\n",
    "    if plot:\n",
    "        plt.figure(figsize=(9, 9))\n",
    "        sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='Blues_r');\n",
    "        plt.ylabel('Actual label');\n",
    "        plt.xlabel('Predicted label');\n",
    "        #all_sample_title = 'Confusion matrix \\n Accuracy Score: {0}\\n {1} {2}'.format(score, previous.version[0], current.version[1])\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        #plt.title(all_sample_title, size=15);\n",
    "    cmm = metrics.multilabel_confusion_matrix(y_test, y_predicted)\n",
    "    if verbose:\n",
    "        print(\"Confusion multiclass matrix :\\n\", cmm)\n",
    "        print(\"CLASSIFICATION REPORT\")\n",
    "        print(classification_report(y_test, y_predicted))\n",
    "        print(\"Accuracy:\", metrics.accuracy_score(y_test, y_predicted))\n",
    "\n",
    "    \n",
    "############################################################################\n",
    "############################################################################\n",
    "#    'micro':\n",
    "#     Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "#\n",
    "#   'macro':\n",
    "#     Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "############################################################################\n",
    "############################################################################\n",
    "\n",
    "    gmean = geometric_mean_score(y_test, y_predicted, average = 'micro')\n",
    "    #print(cm)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)\n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    #TPR = TP / (TP + FN)\n",
    "    TPR = recall_score(y_test, y_predicted, average='micro')\n",
    "\n",
    "    # Fall out or false positive rate\n",
    "    #FPR = FP / (FP + TN)\n",
    "    FPR = 1 - (specificity_score(y_test, y_predicted, average='micro'))\n",
    "    \n",
    "    #Balance\n",
    "    balance = 1 - (np.sqrt((0 - FPR) ** 2 + (1 - TPR) ** 2) / np.sqrt(2))\n",
    "    balance = np.average(balance)\n",
    "\n",
    "    ##F MEASURE##\n",
    "    fmeasure = f1_score(y_test, y_predicted, average = 'micro')\n",
    "\n",
    "    if verbose:\n",
    "        print('TPR :', TPR)\n",
    "        print('FPR :', FPR)\n",
    "        \n",
    "    print('F-Measure : ', fmeasure)\n",
    "    print('G-Mean :', gmean)\n",
    "    print('Balance :', balance)\n",
    "\n",
    "    '''filename = join(c.MODEL_DIR , 'digits_classifier.joblib.pkl')\n",
    "    _ = joblib.dump(clf, filename, compress=9)\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_run(verbose = False):\n",
    "    global mappednoembeddataset\n",
    "    ant_csv = glob.glob(mappednoembeddataset)\n",
    "    ant_token = []\n",
    "    for i in ant_csv:\n",
    "        tmp = pd.read_csv(i, names=['pathfolder', 'label'])\n",
    "        ant_token.append(tmp)\n",
    "    if verbose:\n",
    "        print(ant_csv)\n",
    "    ant_token = []\n",
    "    for i in ant_csv:\n",
    "        tmp = pd.read_csv(i, names=['pathfolder', 'label'])\n",
    "        ant_token.append(tmp)\n",
    "    ant_token =pd.concat(ant_token)\n",
    "    versions = [i.split('\\\\')[2][:-4] for i in ant_csv]\n",
    "    version_tuple = [(x, y) for x, y in zip(versions[0::1],versions[1::1])]\n",
    "    if verbose:\n",
    "        print(version_tuple)\n",
    "    \n",
    "    for v in version_tuple:\n",
    "        print(v)\n",
    "        generate_token(ant_token, v, verbose = verbose)\n",
    "        #break\n",
    "#     max_length, words = convert_words(versions)\n",
    "    \n",
    "#     do_new_embedded()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappeddataset = c.mapped_data+'/camel*.csv'\n",
    "classicdataset = c.camelCNN\n",
    "mappednoembeddataset = c.mappednoembed_data + '/camel*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('camel-1.2', 'camel-1.4')\n",
      "Conver words starting...\n",
      "convert_words finish\n",
      "Conver words starting...\n",
      "Create new embedded files starting...\n",
      "do_new_embedded finish\n",
      "finish\n",
      "Create Dataset For CNN starting...\n",
      "Create the CNN Model\n",
      "Generate Sematic Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roxil\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataset for Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\G\\progetto-tesi\\Utils\\functions.py:31: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  for (a, b) in zip(A, B)])\n",
      "D:\\G\\progetto-tesi\\Utils\\functions.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  for (a, b) in zip(A, B)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Regressor...\n",
      "[ 0  0  0  0  2  0  0  1  0  0  0  0  2  0  2  1  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  2  0  2  0  0  0  0  0  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  3  0  0  0\n",
      "  0  0  0  0  2  0  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0  0  2  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  3  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  3 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  3  0  0  3  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  1  3  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  7  4  0  1  4  4  0  0\n",
      "  0  0  3  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  1  0  0  3  0  0  1  0  0  0  0  0  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  0  2  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  3  0  0  0  0\n",
      "  0  0  3  0  0  0  0  0  0  0  0  1  0  0  0  0 28  0  3  0  1  0  3  0\n",
      "  0  0  0  0  0  0  0  0  0  0  3  0  0  0  0  0  1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  2  0  0  9  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  0  0]\n",
      "- LogisticRegression score: 0.7910271546635183\n",
      "F-Measure :  0.7910271546635183\n",
      "G-Mean : 0.8822196576022271\n",
      "Balance : 0.8517973509271781\n",
      "Finished\n",
      "('camel-1.4', 'camel-1.6')\n",
      "Conver words starting...\n",
      "convert_words finish\n",
      "Conver words starting...\n",
      "Create new embedded files starting...\n",
      "do_new_embedded finish\n",
      "finish\n",
      "Create Dataset For CNN starting...\n",
      "Create the CNN Model\n",
      "Generate Sematic Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roxil\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataset for Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\G\\progetto-tesi\\Utils\\functions.py:31: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  for (a, b) in zip(A, B)])\n",
      "D:\\G\\progetto-tesi\\Utils\\functions.py:31: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  for (a, b) in zip(A, B)])\n"
     ]
    }
   ],
   "source": [
    "DS3 = True\n",
    "\n",
    "\n",
    "start_run(verbose = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
